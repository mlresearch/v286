---
title: 'Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based
  Text Classifiers'
abstract: Transformers have profoundly influenced AI research, but explaining their
  decisions remains challenging – even for relatively simpler tasks such as classification
  – which hinders trust and safe deployment in real-world applications. Although activation-based
  attribution methods effectively explain transformer-based text classification models,
  our findings reveal that these methods can be undermined by class-irrelevant features
  within activations, leading to less reliable interpretations. To address this limitation,
  we propose Contrast-CAT, a novel activation contrast-based attribution method that
  refines token-level attributions by filtering out class-irrelevant features. By
  contrasting the activations of an input sequence with reference activations, Contrast-CAT
  generates clearer and more faithful attribution maps. Experimental results across
  various datasets and models confirm that Contrast-CAT consistently outperforms state-of-the-art
  methods. Notably, under the MoRF setting, it achieves average improvements of $\times
  1.30$ in AOPC and $\times 2.25$ in LOdds over the most competing methods, demonstrating
  its effectiveness in enhancing interpretability for transformer-based text classification.
openreview: vaQMMxhCqy
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: han25a
month: 0
tex_title: 'Contrast-CAT: Contrasting Activations for Enhanced Interpretability in
  Transformer-based Text Classifiers'
firstpage: 1616
lastpage: 1625
page: 1616-1625
order: 1616
cycles: false
bibtex_author: Han, Sungmin and Lee, Jeonghyun and Lee, Sangkyun
author:
- given: Sungmin
  family: Han
- given: Jeonghyun
  family: Lee
- given: Sangkyun
  family: Lee
date: 2025-07-11
address:
container-title: Proceedings of the Forty-first Conference on Uncertainty in Artificial
  Intelligence
volume: '286'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 11
pdf: https://raw.githubusercontent.com/mlresearch/v286/main/assets/han25a/han25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
