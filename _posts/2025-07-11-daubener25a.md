---
title: ELBO, regularized maximum likelihood, and their common one-sample approximation
  for training stochastic neural networks
abstract: Monte Carlo approximations are central to the training of stochastic neural
  networks in general, and Bayesian neural networks (BNNs) in particular. We observe
  that the common one-sample approximation of the standard training objective can
  be viewed both as maximizing the Evidence Lower Bound (ELBO) and as maximizing a
  regularized log-likelihood of a compound distribution. This latter approach differs
  from the ELBO only in the order of the logarithm and expectation, and is theoretically
  grounded in PAC-Bayes theory. We argue theoretically and demonstrate empirically
  that training with the regularized maximum likelihood increases prediction variance,
  enhancing performance in misspecified settings, adversarial robustness, and strengthening
  out-of-distribution (OOD) detection. Our findings help reconcile previous contradictions
  in the literature by providing a detailed analysis of how training objectives and
  Monte Carlo sample sizes affect uncertainty quantification in stochastic neural
  networks.
openreview: j4yFTNrdrO
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: daubener25a
month: 0
tex_title: ELBO, regularized maximum likelihood, and their common one-sample approximation
  for training stochastic neural networks
firstpage: 897
lastpage: 914
page: 897-914
order: 897
cycles: false
bibtex_author: D\"{a}ubener, Sina and Damm, Simon and Fischer, Asja
author:
- given: Sina
  family: DÃ¤ubener
- given: Simon
  family: Damm
- given: Asja
  family: Fischer
date: 2025-07-11
address:
container-title: Proceedings of the Forty-first Conference on Uncertainty in Artificial
  Intelligence
volume: '286'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 11
pdf: https://raw.githubusercontent.com/mlresearch/v286/main/assets/daubener25a/daubener25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
