---
title: Fast Non-convex Matrix Sensing with Optimal Sample Complexity
abstract: We study the problem of recovering an unknown $d_1 \times d_2$ rank-$r$
  matrix from $m$ random linear measurements. Convex methods achieve the optimal sample
  complexity $m = \Omega(r(d_1 + d_2))$ but are computationally expensive. Non-convex
  approaches, while more computationally efficient, often require suboptimal sample
  complexity $m = \Omega(r^2(d_1 + d_2))$. Recent advance achieves $m = \Omega(rd_1)$
  for a non-convex approach but relies on the restrictive assumption of positive semidefinite
  (PSD) matrices and suffers from slow convergence in ill-conditioned settings. Bridging
  this gap, we show that Riemannian gradient descent (RGD) achieves both optimal sample
  complexity and computational efficiency without requiring the PSD assumption. Specifically,
  for Gaussian measurements, RGD exactly recovers the low-rank matrix with $m = \Omega(r(d_1
  + d_2))$, matching the information-theoretic lower bound, and converges linearly
  to the global minimum with an arbitrarily small convergence rate.
openreview: uADP39Pjhv
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cai25b
month: 0
tex_title: Fast Non-convex Matrix Sensing with Optimal Sample Complexity
firstpage: 497
lastpage: 520
page: 497-520
order: 497
cycles: false
bibtex_author: Cai, Jian-Feng and Wu, Tong and Xia, Ruizhe
author:
- given: Jian-Feng
  family: Cai
- given: Tong
  family: Wu
- given: Ruizhe
  family: Xia
date: 2025-07-11
address:
container-title: Proceedings of the Forty-first Conference on Uncertainty in Artificial
  Intelligence
volume: '286'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 11
pdf: https://raw.githubusercontent.com/mlresearch/v286/main/assets/cai25b/cai25b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
