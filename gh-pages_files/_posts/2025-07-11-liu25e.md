---
title: Periodical Moving Average Accelerates Gradient Accumulation for Post-Training
abstract: 'High gradient variance presents a significant obstacle to efficient post-training
  of large language models (LLMs) on memory-constrained devices. Existing practical
  strategies-such as reducing batch sizes or adopting gradient accumulation (GA)-suffer
  from an inherent trade-off: smaller batches exacerbate convergence issues due to
  increased gradient noise, while GA substantially prolongs training time owing to
  its sequential processing. In this work, we reveal that the Exponential Moving Average
  (EMA) in momentum-based optimizers exponentially discounts historical gradients,
  thereby limiting their effectiveness in stabilizing parameter updates, especially
  during post-training when parameter drift is minimal. Motivated by this, we propose
  integrating the core idea of GA directly into momentum updates via a novel Periodical
  Moving Average (PMA) mechanism, which structures training into fixed periods and
  replaces EMA with a uniform moving average within each period. We instantiate PMA
  within AdamW and Lion, resulting in the AdamW-PMA and Lion-PMA optimizers. Theoretical
  analysis establishes that AdamW-PMA matches the convergence guarantees of standard
  Adam. Extensive empirical evaluation on supervised fine-tuning and direct preference
  optimization tasks demonstrates that PMA-based methods achieve approximately $2\times$
  faster training compared to GA, while yielding consistently better performance on
  downstream evaluations.'
software: https://github.com/liuyumou/periodical-moving-average.git
openreview: iAfRDuiTjd
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu25e
month: 0
tex_title: Periodical Moving Average Accelerates Gradient Accumulation for Post-Training
firstpage: 2748
lastpage: 2768
page: 2748-2768
order: 2748
cycles: false
bibtex_author: Liu, Yumou and Li, An and Li, Chaojie and Yu, Fei and Wang, Benyou
author:
- given: Yumou
  family: Liu
- given: An
  family: Li
- given: Chaojie
  family: Li
- given: Fei
  family: Yu
- given: Benyou
  family: Wang
date: 2025-07-11
address:
container-title: Proceedings of the Forty-first Conference on Uncertainty in Artificial
  Intelligence
volume: '286'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 11
pdf: https://raw.githubusercontent.com/mlresearch/v286/main/assets/liu25e/liu25e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
