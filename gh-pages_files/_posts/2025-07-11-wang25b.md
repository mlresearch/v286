---
title: Targeted Learning for Variable Importance
abstract: Variable importance is one of the most widely used measures for interpreting
  machine learning with significant interest from both statistics and machine learning
  communities. Recently, increasing attention has been directed toward uncertainty
  quantification in these metrics. Current approaches largely rely on one-step procedures,
  which, while asymptotically efficient, can present higher sensitivity and instability
  in finite sample settings. To address these limitations, we propose a novel method
  by employing the targeted learning (TL) framework, designed to enhance robustness
  in inference for variable importance metrics. Our approach is particularly suited
  for conditional permutation variable importance. We show that it (i) retains the
  asymptotic efficiency of traditional methods, (ii) maintains comparable computational
  complexity, and (iii) delivers improved accuracy, especially in finite sample contexts.
  We further support these findings with numerical experiments that illustrate the
  practical advantages of our method and validate the theoretical results.
openreview: q8tzRNPwFk
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang25b
month: 0
tex_title: Targeted Learning for Variable Importance
firstpage: 4395
lastpage: 4410
page: 4395-4410
order: 4395
cycles: false
bibtex_author: Wang, Xiaohan and Zhou, Yunzhe and Hooker, Giles
author:
- given: Xiaohan
  family: Wang
- given: Yunzhe
  family: Zhou
- given: Giles
  family: Hooker
date: 2025-07-11
address:
container-title: Proceedings of the Forty-first Conference on Uncertainty in Artificial
  Intelligence
volume: '286'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 11
pdf: https://raw.githubusercontent.com/mlresearch/v286/main/assets/wang25b/wang25b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
