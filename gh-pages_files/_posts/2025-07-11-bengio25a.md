---
title: Can a Bayesian Oracle Prevent Harm from an Agent?
abstract: Is there a way to design powerful AI systems based on machine learning methods
  that would satisfy probabilistic safety guarantees? With the long-term goal of obtaining
  a probabilistic guarantee that would apply in every context, we consider estimating
  a context-dependent bound on the probability of violating a given safety specification.
  Such a risk evaluation would need to be performed at run-time to provide a guardrail
  against dangerous actions of an AI. Noting that different plausible hypotheses about
  the world could produce very different outcomes, and because we do not know which
  one is right, we derive bounds on the safety violation probability predicted under
  the true but unknown hypothesis. Such bounds could be used to reject potentially
  dangerous actions. Our main results involve searching for cautious but plausible
  hypotheses, obtained by a maximization that involves Bayesian posteriors over hypotheses.
  We consider two forms of this result, in the i.i.d. case and in the non-i.i.d. case,
  and conclude with open problems towards turning such theoretical results into practical
  AI guardrails.
software: https://github.com/saifh-github/conservative-bayesian-public
openreview: ZfeRS4kGAA
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bengio25a
month: 0
tex_title: Can a Bayesian Oracle Prevent Harm from an Agent?
firstpage: 257
lastpage: 270
page: 257-270
order: 257
cycles: false
bibtex_author: Bengio, Yoshua and Cohen, Michael K. and Malkin, Nikolay and MacDermott,
  Matt and Fornasiere, Damiano and Greiner, Pietro and Kaddar, Younesse
author:
- given: Yoshua
  family: Bengio
- given: Michael K.
  family: Cohen
- given: Nikolay
  family: Malkin
- given: Matt
  family: MacDermott
- given: Damiano
  family: Fornasiere
- given: Pietro
  family: Greiner
- given: Younesse
  family: Kaddar
date: 2025-07-11
address:
container-title: Proceedings of the Forty-first Conference on Uncertainty in Artificial
  Intelligence
volume: '286'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 11
pdf: https://raw.githubusercontent.com/mlresearch/v286/main/assets/bengio25a/bengio25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
